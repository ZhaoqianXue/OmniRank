# Automated Hypothesis Validation with Agentic Sequential Falsifications

## Abstract

Hypotheses are central to information acquisition, decision-making, and discovery. However, many real-world hypotheses are abstract, high-level statements that are difficult to validate directly. This challenge is further intensified by the rise of hypothesis generation from Large Language Models (LLMs), which are prone to hallucination and produce hypotheses in volumes that make manual validation impractical. Here we propose POPPER, an agentic framework for rigorous automated validation of free-form hypotheses. Guided by Karl Popper’s principle of falsification, POPPER validates a hypothesis using LLM agents that design and execute falsification experiments targeting its measurable implications. A novel sequential testing framework ensures strict Type-I error control while actively gathering evidence from diverse observations, whether drawn from existing data or newly conducted procedures. We demonstrate POPPER on six domains including biology, economics, and sociology. POPPER delivers robust error control, high power, and scalability. Furthermore, compared to human scientists, POPPER achieved comparable performance in validating complex biological hypotheses while reducing time by 10 folds, providing a scalable, rigorous solution for hypothesis validation. POPPER is freely available at [https://github.com/snap-stanford/POPPER](https://github.com/snap-stanford/POPPER).

## 1. Introduction

A hypothesis is a theory or an explanation based on limited evidence. It forms the backbone of decision-making, information acquisition, and discovery across domains (Thompson & Skau, 2023). For example, a robot evaluates different hypotheses to decide what action to take next. A scientist decides which experiments to run to evaluate a hypothesis/theory. The marketing strategy decisions are guided by the hypothesized effect on increasing customer retention. Similarly, policymakers may rely on hypotheses about the outcomes of proposed interventions.

Given their profound implications, it is important to validate hypotheses with supporting evidence. This need has grown increasingly urgent with the recent surge in hypotheses generated by Large Language Models (LLMs) (Wang et al., 2024b; Zhou et al., 2024). While these systems exhibit remarkable creativity and diversity, the plausibility of their generated hypotheses can vary significantly due to potential hallucinations (Huang et al., 2023). Moreover, the sheer volume of LLM-generated hypotheses makes it impractical to invest in each one immediately. Therefore, obtaining a reliable, scalable understanding of the quality of these hypotheses is essential to fully unlock their potential.

Having said this, many real-world hypotheses are abstract natural language statements that are difficult to directly evaluate (Thompson & Skau, 2023; Godfrey-Smith, 2009). For example, while we might hypothesize that “a gene causes a disease,” it is infeasible to test this statement directly as it stands. Instead, it must be translated into specific, measurable implications that can be experimented rigorously (Jun et al., 2022). Yet, even for a single hypothesis, the space of potential supportive implications is vast, highlighting the need for frameworks that can automate this evaluation process. Notably, such frameworks must also be statistically rigorous, avoiding false verifications of hypotheses that are not true (Neyman & Pearson, 1928; 1933; Fisher, 1936). Without such control, research efforts risk being misdirected, resources wasted, and harmful conclusions drawn, ultimately undermining progress and trust. Overall, this raises a critical question: *How can we rigorously validate free-form hypotheses at scale?*

**Present work.** We introduce POPPER, a novel framework for rigorous and automated validation of free-form natural language hypotheses using LLM agents. Inspired by Karl Popper’s principle of falsification (Popper, 2005), POPPER systematically challenges hypotheses by sequentially testing their measurable implications through diverse experiments, ranging from data analysis and simulations to real-world experiments and interventions.

To automate this process, POPPER employs two specialized LLM agents with complementary roles. The *Experiment Design Agent* leverages reasoning capabilities and domain knowledge to identify a measurable implication (sub-hypothesis) of the main hypothesis and design a falsification experiment. Notably, this sub-hypothesis needs to be falsifiable with clear null and alternative definitions. Once designed, the *Experiment Execution Agent* implements the experiments, which may involve data collection, simulations, statistical analyses, or real-world procedures. This agent ultimately produces a p-value that summarizes the outcome of the falsification experiment.

To maintain statistical rigor, POPPER introduces a novel sequential testing framework that aggregates evidence from multiple, potentially dependent LLM-generated tests while strictly controlling the Type-I error rate (i.e., the probability of incorrectly rejecting a true null hypothesis). Individual p-values are converted into e-values (Vovk & Wang, 2021), enabling the aggergation of cumulative evidence. By adaptively combining these e-values, POPPER determines whether to reject the hypothesis, conduct further experiments, or terminate the validation process. The framework’s ability to make dynamic, statistically sound decisions is ensured by the *any-time validity* property of the combined e-values (Grünwald et al., 2020). By iteratively testing adaptively solicited implications of a hypothesis, POPPER systematically explores its flexibility while adhering to rigorous statistical principles. This provides a scalable and automated approach to hypothesis validation.

We instantiated POPPER across six diverse domains, including biology, sociology, and economics. In our implementation, POPPER designs falsification experiments by leveraging large-scale, hypothesis-free datasets and executes them with a Python code environment. The process involves systematic data identification, preprocessing, analysis, and statistical evaluation, ultimately generating sequentially valid p-values. Our results demonstrate that POPPER effectively controls the Type-I error rate while achieving significant power improvements over existing methods. Additionally, an expert user study involving nine PhD-level biostatisticians and computational biologists found that POPPER matched human performance in hypothesis validation tasks while reducing validation time by an order of magnitude.

## 2. POPPER: a general framework for automated hypothesis validation

### 2.1. Background and Problem Formulation

Following Majumder et al. (2024); Thompson & Skau (2023), we broadly define hypothesis $H$ as a statement that defines relationships ($r$) between a set of variables ($\mathcal{V}$) under contexts ($c$). For example, in the hypothesis $H$ “Gene VAV1 regulates IL2 production in immune tissue”, $\mathcal{V}$= {“VAV1”, “IL2 production”}, $r$ = “regulate”, and $c$ = “in the immune tissue”. To formalize the discussion, the hypothesis $H$ is associated with a null hypothesis $H_0$. $H_0$ describes a family $\mathcal{P}_0$ of distributions that generate the data under the null, i.e., in uninteresting situations (such as “Gene VAV1 does not regulate IL2 production”). In this way, $H_0$ being incorrect is of interest (the alternative hypothesis). Hypothesis validation aims to test the null hypothesis $H_0$ and suggest evidence for the alternative.

The hypothesis validation task is defined as $f : H \rightarrow \{0, 1\}$, where 0 stands for unvalidated and 1 stands for validated (claiming the alternative). Given a hypothesis $H$, a system or a program $f$ designs and performs experiments and generates an answer in $\{0, 1\}$. We denote $\hat{y}$ as the predicted validation status. An experiment is typically associated with the collection (or retrieval) and processing of datasets denoted as $\mathcal{D}$. An LLM agent $A$ is broadly defined as a program that takes in instructions in natural language and performs actions $\mathcal{T}$ with reasoning capabilities to solve the task given the instruction and outputs a natural language answer (Yao et al., 2023).

For rigorous hypothesis validation, we adopt the classical Type-I error control as our primary criterion. The Type-I error is the probability of the system incorrectly claiming an “interesting” finding (e.g., enriched gene expression) when the null hypothesis is true. Formally, the Type-I error rate is $\sup_{\mathbb{P}\in\mathcal{P}_0} \mathbb{P}(\hat{y} = 1)$, where the probability is over the data and the validation system. To ensure rigor, our goal is to control the Type-I error at a pre-defined level $\alpha \in (0, 1)$. Another important concept is the power of the validation system, which we define as $\mathbb{P}(\hat{y} = 1)$ where $\mathbb{P}$ is the data distribution. While power–the ability to detect true effects–is important, its improvement is meaningful only when Type-I error control is ensured. Without this foundation, increased power risks invalid conclusions.

### 2.2. Overview of POPPER

POPPER is an agentic framework to systematically validate a hypothesis by actively designing and executing a sequence of *falsification experiments*. This perspective is inspired by Karl Popper’s philosophy of falsification (Popper, 2005): rather than trying to directly prove a hypothesis of interest, one can attempt to *refute* its logical implications through experiments.

Suppose we want to investigate whether gene $X$ is related to disease $Y$. Directly establishing such a relationship may be difficult; however, we can test one of its implications: if $X$ truly has no relationship to $Y$, we might expect no significant difference in $X$’s expression levels when comparing cell types implicated in $Y$ versus unrelated cell types.

**Figure 1: Illustration of POPPER.** Given a hypothesis and a pre-defined significance level $\alpha \in (0, 1)$, POPPER constructs sequential experiments to falsify the hypothesis. Each iteration proceeds as follows. First, an experiment design agent proposes a falsification experiment, which is refined through a self-critique process considering factors such as causality, data availability, and redundancy. The experiment is then evaluated by an LLM-as-a-judge relevance checker to ensure its alignment with the main hypothesis. If deemed relevant, the test is implemented by a ReAct-based experiment execution agent which obtains a p-value. P-values from multiple falsification experiments are aggregated into sequential e-values using a sequential testing framework. If the aggregated e-value exceeds $1/\alpha$, we declare sufficient evidence to reject the null hypothesis. Otherwise, the process continues with the next falsification test.

Hence, a potential falsification experiment is to measure expression for $X$, collect the relevant samples, and apply a statistical test (e.g. a t-test) for the null hypothesis that there is no difference in mean expression. In this sense, each experimental design leverages a logical implication of the main hypothesis to gather evidence. One can design multiple experiments like this to refute the primary hypothesis.

POPPER implements an iterative, LLM-driven framework for systematic falsification. At each round $i$, an *experiment design agent* proposes a falsification test for a sub-hypothesis $h^0_i$ (e.g., “no difference in expression”), based on the main hypothesis and available resources. An *experiment execution agent* then carries out the test - either by analyzing existing data, conducting lab measurements, or running simulations - and reports a p-value $p_i$. A *sequential error control* step converts $p_i$ into an e-value $e_i$ (detailed in Section 2.3), ensuring statistically valid accumulation of evidence. This process repeats over multiple iterations, collecting e-values until either (i) the aggregated evidence surpasses a predefined threshold, leading to a rejection of the null hypothesis $H_0$, or (ii) a maximum number of iterations is reached. Each experiment may involve real-world data collection or simulations. The only restriction is that it produces a valid p-value suitable for e-value computation under the specified null sub-hypothesis. Next, we formalize the theoretical underpinnings of this sequential approach and provide descriptions of the POPPER framework.

### 2.3. Validity of Type-I Error Control in POPPER

This part lays out the general conditions needed for valid Type-I error control in POPPER.

**Assumption 1 (Implication).** If $H_0$ is true, then the null sub-hypothesis $h^0_i$ is true for all $i \ge 1$.

Assumption 1 requires that the null sub-hypothesis $h^0_i$ describes a range of data generating processes that are contained in those described by $H_0$. As we are to detail in Section 3, we leverage the reasoning capabilities of LLMs, as well as additional checks to overcome the intrinsic randomness in LLM agents to approximately fulfill this condition.

Recall that an e-value $e_i \in \mathbb{R}$ is computed based on the collected data in each iteration. Following Vovk & Wang (2021), an e-value is a non-negative random variable whose expectation is below 1 under the null hypothesis and such that if it were to take a large value, it would indicate strong evidence for refuting the null. E-values are our key instruments for Type-I error control. Compared with the classical concept of p-values, their advantages include (i) flexible combination of evidence$^1$ and (ii) adaptive stopping of the validation process (Grünwald et al., 2020). Let $\mathcal{D}$ be the data, POPPER could potentially interact with (including yet-to-collect ones). To achieve these benefits, in POPPER we require the e-values to be *sequentially* valid.

$^1$Traditional methods like Fisher’s combined test (Fisher, 1970) or Brown’s method (Brown, 1975) rely on strong assumptions such as independent p-values or accurate modeling. They also cannot ensure Type-I error control with optional stopping (Assumption 3).

**Assumption 2 (Sequential information).** The training process of the agents is independent of $\mathcal{D}$. Let $\mathcal{D}_{i-1} := \{D_s\}_{s \le i-1}$ be the datasets used by the agents before iteration $i$. The e-values obey $\mathbb{E}[e_i \mid \mathcal{D}_{i-1}] \le 1$ under $h^0_i$.

Assumption 2 requires that the e-value at each iteration is valid conditional on prior information. As we shall see in Section 3, POPPER achieves this by carefully controlling the information used at each iteration. In specific, suppose at iteration $i$, the agents determine a sub-hypothesis $h^0_i$ and a test function $f_i(\cdot)$, and then compute $e_i = f_i(D_i)$ based on a collected dataset $D_i$ (e.g. through transforming a p-value). Then, Assumption 2 holds if (1) the selection of $h^0_i$, and $f_i(\cdot)$ only relies on $\mathcal{D}_{i-1}$ and metadata without involving the samples in the unused, yet-to-be-chosen datasets, and (2) $\mathbb{E}[f(D)] \le 1$ for any fixed value of $h$ (resp. $f$) that $f_i$ (resp. $h^0_i$) may take and any dataset $D$ whose distribution obeys $h$. If $D_i$ is a dataset from a static database, then condition (1) means the decision of using $D_i$ does not involve the data in it; if $D_i$ is actively collected, then (1) is natural as the data must be collected after the design stage.

The last assumption concerns the stopping rule of the validation process. It ensures that the aggregated evidence at the terminal iteration supports rigorous validation outputs.

**Assumption 3 (Optional stopping).** The random variable $\tau \in \mathbb{N}^+$ denoting the termination iteration is a stopping time with respect to the filtration $\mathcal{F}_i = \sigma(\mathcal{D}_i)$. That is, for every $i$, the event $\{\tau = i\}$ is measurable with respect to $\mathcal{F}_i$.

Assumption 3 holds if the decision to stop or continue at iteration $i$ only depends on $\mathcal{D}_i$. In POPPER, we determine termination through the aggregated evidence $E_i := \prod_{s=1}^i e_s$.

These assumptions ensure the aggregated evidence $\{E_i\}$ is a super-martingale (also called e-process (Shafer, 2019; Grünwald et al., 2020)), and thus the $E_i$ at the terminal step can be used to produce the validation output with error control. Theorem 4 is a standard result following (Grünwald et al., 2020), proved in Appendix A.2 for completeness.

**Theorem 4.** *Define the aggregated evidence at the termination iteration as $E := \prod_{s=1}^\tau e_s$. Under Assumptions 1, 2 and 3, $E$ is a valid e-value, i.e., $\mathbb{E}[E] \le 1$ under $H_0$. In addition, define the validation status as $\hat{y} = \mathbf{1}\{E \ge 1/\alpha\}$. Then, $\mathbb{P}(\hat{y} = 1) \le \alpha$ under $H_0$, where the probability $\mathbb{P}$ is over the randomness in the agents and the collected data.*

### 2.4. Agentic hypothesis validation framework

We now introduce each component of POPPER in a general form. Although the particular implementation we showcase later uses a static database, POPPER can be deployed in *any* environment capable of producing valid p-values - whether that involves laboratory experiments, real-time data collection, or computational simulations. The essence is to iteratively design and execute *falsification experiments* on sub-hypotheses derived from a main hypothesis $H$. Below, we describe how our agents accomplish this while maintaining the assumptions needed for Type-I error control.

**Experiment design agent.** Given the main hypothesis $H$ and history of previously tested sub-hypotheses (and their outcomes), the *design agent* proposes a new falsification experiment intended to refute $H_0$. Concretely, it specifies:
*   A *sub-hypothesis* capturing a concrete implication of the main hypothesis.
*   The *null* $h^0_i$ and *alternative* $h^1_i$ to be tested.
*   Details of how to conduct the experiment in a given domain. This may involve recommending the collection of new laboratory samples, setting up a targeted simulation, or identifying a suitable dataset (if available).

The design agent is assumed to have domain expertise or access to domain knowledge, allowing it to propose experiments that are both *relevant* for falsifying $H_0$ and *feasible* to implement. For instance, it might propose measuring gene-expression levels, or running a randomized simulation study, or analyzing an existing database - whatever is best to challenge the null sub-hypothesis. Critically, the design agent must ensure that the proposed experiment can, in principle, yield a valid p-value under $h^0_i$. We will later show how this agent’s operations are automated in practice in Section 3.

**Experiment execution agent.** Once an experiment is designed, it is handed off to the *execution agent*, which is responsible for carrying it out. In a laboratory setting, this agent might interface with robotic lab equipment or prompt human technicians to conduct the specified protocol. In a simulation, it would set up and run the relevant computational model. In a data analytics context, it would query and analyze the dataset. Regardless of the experimental modality, the only restriction is that it outputs a valid p-value under $h^0_i$ (Assumption 2). If an experiment fails - because the protocol cannot be completed or the data are insufficient - it is simply recorded as a failed attempt, and the procedure moves on. In Section 3, we show how this agent is instantiated using a code-generation framework that automatically executes data queries and statistical analyses.

**Sequential aggregation of statistics for error control.** After obtaining the new p-value $p_i$, we aggregate existing falsification tests to collectively measure evidence for the main hypothesis while maintaining Type-I error control. As described in the proposed sequential testing framework in Section 2.3, the main technical tools we use are e-values (Vovk & Wang, 2021), which are amenable to combination of evidence and adaptive decisions to continue or not (safe testing) (Grünwald et al., 2020). Many e-value constructions (e.g. likelihood ratios) require modeling assumptions, which are unsuitable given the flexibility given to our agent. Thus, we use the general “p-to-e calibrator” (Vovk & Wang, 2021) to construct

$$ e_i = \kappa \times p_i^{\kappa-1}, \quad \kappa \in (0, 1). \quad (1) $$

It is straightforward to check that $\mathbb{E}[e_i \mid \mathcal{D}_{i-1}] \le 1$ if each $p_i$ is a conditionally valid p-value, i.e., $\mathbb{P}(p_i \le t \mid \mathcal{D}_{i-1}) \le t$ for any $t \in [0, 1]$. We then compute the aggregated evidence $E_i = \prod_{s=1}^i e_s$. If $E_i \ge 1/\alpha$, then $H_0$ is rejected and $H$ is verified (obeying Assumption 3). If not, we proceed to the next iteration until a budget is reached. Theorem 4 ensures the Type-I error control of this procedure.

## 3. Instantiation of POPPER

Thus far, we have described POPPER as a general, agentic framework capable of executing any type of experiment - laboratory procedures, simulations, or data analyses - to test sub-hypotheses under a unifying Popperian falsification paradigm. In this section, we focus on our current *instantiation*, where experiments are drawn from a static corpus of massive hypothesis-free datasets ($\mathcal{D}$) rather than real-world or real-time data acquisition. We emphasize that this is only one possible deployment of POPPER, chosen here for ease of implementation and reproducibility.

**Domains and hypotheses.** Our demonstration uses two collections. The first, *Target Validation (TargetVal)*, addresses genotype-phenotype hypotheses in biology; it aggregates 22 tables (totaling $\sim$ 85 million records) from sources such as GTEx (Consortium, 2020), GWAS Catalog (MacArthur et al., 2017), and BioGrid (Oughtred et al., 2019). Hypotheses in TargetVal follow the template “Gene A regulates Phenotype B,” and we assess them using two subtasks: Interleukin-2 (TargetVal-IL2) and Interferon-gamma (TargetVal-IFNG). Ground-truth hypotheses (treated as “positive” references) were approximated based on genome-wide CRISPR screen data (Schmidt et al., 2022). The second, *DiscoveryBench* (Majumder et al., 2024), spans six domains (sociology, biology, humanities, economics, engineering, and meta-science), yielding 86 non-null hypotheses (after deduplication) that are grounded in peer-reviewed research. Each hypothesis is paired with a set of relevant dataset. In all cases, POPPER is provided only with the high-level *schema* (row and column names, any available short text descriptions) of each dataset and the main hypothesis $H$. It must then propose and implement sub-hypothesis falsification experiments by querying and analyzing the raw data.

**Instantiation of the experiment design agent.** At iteration $i$, the *Design Agent* $A_{\text{design}}$ receives the main hypothesis $H$, previously proposed falsification sub-hypotheses $\{h_1, \dots, h_{i-1}\}$, their corresponding p-values $\{p_1, \dots, p_{i-1}\}$, and the metadata from the database $\mathcal{D}$, and then intelligently designs a new falsification experiment with sub-hypothesis $h_i$. To ensure robustness, $A_{\text{design}}$ operates under metadata-only access, meaning it sees only the *schema* of each table but has no access to raw data or summary statistics, thereby satisfying Assumption 2. In the experiment proposal step, the agent generates a concise rationale, along with a null hypothesis $h^0_i$ and an alternative hypothesis $h^1_i$. To enhance quality, we incorporate Self-Refinement (Madaan et al., 2024), employing a chain-of-thought approach that prompts the LLM to iteratively improve its proposal based on three key criteria: novelty (avoiding redundant sub-hypotheses), implementability (ensuring feasibility given metadata), and logical relevance (confirming that $H$ implies $h_i$). A real-world example is illustrated in Table 1. This demonstrates the agent’s ability to systematically design rigorous and biologically meaningful experiments, highlighting its effectiveness in guiding the falsification process. A detailed analysis of the proposed experiments is available at Section 4.2.

**Table 1: Experiment design example.** Designs for the hypothesis “Gene ZAP70 regulates the production of Interleukin-2”.

| Round | Falsification experiment description generated from POPPER experiment design agent | P-value | Cum. e-value |
| :--- | :--- | :--- | :--- |
| 1 | "Test if ZAP70 has significant physical protein-protein interactions with IL-2 pathway components using affinity capture Mass Spectrometry data" | 1.0 | 0.5✗ |
| 2 | "Test if ZAP70 expression levels correlate with IL-2 pathway genes across tissues using GTEx tissue expression data" | 8.8e-3 | 2.67✗ |
| 3 | "Test if genetic variants affecting ZAP70 expression (eQTLs) are also associated with changes in IL-2 pathway activity in immune cells using UKBB eQTL data" | - | - |
| 4 | "Test if rare missense variants in ZAP70 are significantly associated with immune phenotypes related to IL-2 function using GeneBASS missense variant data" | 4.7e-04 | 30.78✓ |

**Relevance checker.** Even with self-refinement, the *Design Agent* may produce experiments that are tangential to the main hypothesis $H$. To enforce Assumption 1, we introduce a *relevance checker*, an LLM-based function $R(h) \in [0, 1]$ that estimates how strongly the proposed null sub-hypothesis $h$ is implied by $H_0$. If $R(h) < r_0$ (a predefined threshold), we discard that experiment and prompt $A_{\text{design}}$ to propose a new one. This pruning mitigates the risk that an *irrelevant* null might be “falsified,” incorrectly supporting the hypothesis (thus inflating the Type-I error).

**Instantiation of the experiment execution agent.** Once a proposed experiment passes the relevance check, the *Execution Agent* $A_{\text{exec}}$ carries it out by querying and analyzing the raw data in $\mathcal{D}$ to output a p-value. To give the agent flexibility, we provide a coding environment where it can write and run Python scripts using essential libraries including `pandas`, `statsmodels`, and `scipy`. Concretely, we employ ReAct (Yao et al., 2023) where the agent incrementally executes the experiment via a cycle of actions (executing code), observations (inspecting code output), and reasoning based on the observed output. In practice, $A_{\text{exec}}$ typically inspects and retrieves the dataset, performs preprocessing, fixes errors, runs appropriate statistical tests, fits models, and finally summarizes or visualizes the findings. Without explicit prompting, it selects suitable tests (e.g., t-test, chi-squared, Mann-Whitney U-test) based on the data distribution. Table 2 shows an example, and Section 4.2 analyzes the execution steps in detail.

**Table 2: Experiment execution example.** Execution steps for the experiment “Test if variants in the MAK16 locus region show over-representation of immune-trait GWAS associations.” We provide a summarized pseudo-code here for illustration purposes.

| Step | Execution steps description from POPPER experiment execution agent |
| :--- | :--- |
| 1 | Define a helper function to check if a trait is immune-related |
| 2 | Find the MAK16 gene in df_gene_info |
| 3 | Determine gene region bounds on chromosome (100 kb) |
| 4 | Subset df_variant_table for variants in this region |
| 5 | Merge with GWAS catalog |
| 6 | Filter merged results for (a) p-value 5e-8 (b) immune-related traits using helper function in 1 |
| 7 | Perform 500 permutations by randomly selecting a chromosome and a matching-length region, gathering variants, merging with the GWAS catalog, filtering for immune-related traits with p-value 5e-8, and recording the immune-hit count for each permutation. |
| 8 | Compute the empirical p-value |

## 4. Experiments

We evaluate POPPER in terms of Type-I error control, power improvements, expert user studies, ablations, human annotations, and failure analysis.

**Evaluation setup.** We assess Type-I error by creating *negative examples* through random column-wise permutations in each dataset, ensuring the null hypothesis holds. For DiscoveryBench, we generate as many negative examples as positive ones. For the target validation benchmark (with only 20 positives), we create 50 negatives. We measure Type-I error by the proportion of “reject” decisions ($\hat{y} = 1$) on negative examples and Power by the proportion of “reject” decisions on positive examples. We set a nominal Type-I error level $\alpha = 0.1$. Unless noted otherwise, we use Claude-Sonnet-3.5 as our LLM, with a maximum of 3 tests on DiscoveryBench and 5 on target validation (due to more complex hypotheses in the latter scenario).

**Baselines & variations.** We group comparing methods into two categories. (1) *Baselines*. Since this is a novel application with no direct references, we compare against three general-purpose task resolvers: *CodeGen* (Ridnik et al., 2024), which generates code; *ReAct* (Yao et al., 2023), which iteratively combines reasoning and coding; and *Self-refine* (Madaan et al., 2024), which refines CodeGen outputs via a critic. None include specialized mechanisms for statistical rigor. We also evaluated an enhanced *CodeGen-o1* with improved reasoning. (2) *Variations* of POPPER. These include *Fisher*, which uses p-values and Fisher’s combined test (Fisher, 1970) instead of e-values; *LLM-Likelihood Ratio*, which relies on an LLM to estimate the (optimal) likelihood ratio (Zheng et al., 2023) rather than a p-to-e calibrator; *POPPER-NoReleCheck*, omitting the relevance checker; and *POPPER-CodeGen*, which substitutes ReAct with direct code generation for statistical tests.

### 4.1. Results

**POPPER achieves Type-I error control.** Table 3 reports the Type-I error rates and several key observations are in order. First, most baselines fail to consistently control the Type-I error, while POPPER remains below the nominal level across all datasets. This underscores the necessity of principled statistical design in LLM-driven hypothesis validation; without such rigor, the flexibility of LLM agents can inflate Type-I errors. Second, the comparison against Fisher’s combined test highlights the benefits of e-values in aggregating evidence. Third, the LLM-Likelihood Ratio method lacks calibration, overly conservative for TargetVal-IL2 and too liberal for DiscoveryBench and TargetVal-IFNG, illustrating the need for strict statistical control rather than relying solely on LLM-based estimations. Finally, removing the relevance checker (POPPER-NoReleCheck) significantly raises the Type-I error due to irrelevant and misleading tests. Together, these results establish POPPER as a robust framework for agentic hypothesis validation.

**Table 3: Type-I error/power across baselines, variations, ablations, and POPPER.** A method is considered to achieve Type I-error control if the pre-defined threshold falls within 1 standard deviation of the method’s result. For methods that fail to meet this criterion, the power metric is grayed out, as it becomes invalid. Mean and standard deviation for all metrics are calculated from 5 independent runs.

| Method | Type I Error ($\alpha = 0.1$) | | | Power | | |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| | DiscoveryBench | TargetVal-IL2 | TargetVal-IFNG | DiscoveryBench | TargetVal-IL2 | TargetVal-IFNG |
| CodeGen | 0.145±0.031✗ | 0.020±0.014✓ | 0.004±0.009✓ | 0.378±0.066 | 0.140±0.022 | 0.040±0.042 |
| CodeGen (o1) | 0.248±0.015✗ | 0.013±0.012✓ | 0.000±0.000✓ | 0.419±0.028 | 0.250±0.100 | 0.183±0.076 |
| ReAct | 0.078±0.061✓ | 0.000±0.000✓ | 0.000±0.000✓ | 0.383±0.017 | 0.010±0.022 | 0.020±0.045 |
| Self-Refine | 0.117±0.028✗ | 0.100±0.069✓ | 0.067±0.064✓ | 0.476±0.066 | 0.183±0.029 | 0.067±0.064 |
| Fisher Combined Test | 0.311±0.040✗ | 0.264±0.083✗ | 0.173±0.023✗ | 0.741±0.058 | 0.800±0.071 | 0.650±0.050 |
| LLM-Likelihood ratio | 0.152±0.031✗ | 0.016±0.014✓ | 0.180±0.028✗ | 0.428±0.034 | 0.185±0.074 | 0.357±0.132 |
| POPPER-NoReleCheck | 0.134±0.021✗ | 0.340±0.139✗ | 0.300±0.113✗ | 0.610±0.042 | 0.897±0.004 | 0.717±0.126 |
| POPPER-CodeGen | 0.140±0.022✗ | 0.105±0.017✓ | 0.090±0.045✓ | 0.544±0.032 | 0.526±0.133 | 0.450±0.079 |
| **POPPER (Ours)** | 0.103±0.020✓ | 0.082±0.046✓ | 0.085±0.028✓ | **0.638**\*±0.066 | **0.580**\*±0.125 | **0.591**\*±0.069 |

**POPPER has significant power improvement.** Table 3 shows the power across three benchmarks. First, we exclude any method with an uncontrolled Type-I error (gray-shaded in the table), as their power estimates are invalid. Among methods that do control the Type-I error, POPPER consistently achieves the highest power: on DiscoveryBench, it delivers 66.5% greater power than ReAct, and on TargetVal-IL2, it outperforms Self-Refine by a factor of 3.17. This highlights the strength of POPPER’s iterative testing mechanism, which continually accumulates evidence to improve validation. Second, POPPER with the ReAct coding agent outperforms POPPER-CodeGen in power - even with a lower Type-I error. The likely cause is that its reasoning module enables more effective falsification tests. Overall, these results confirm the ability of POPPER to balance high power with error control, making it a reliable and efficient approach to hypothesis validation.

**POPPER compares with human experts.** We recruited nine computational biologists and bioinformaticians (either PhD holders or candidates) to perform hypothesis validation on TargetVal-IL2 (details in Appendix F). Figure 2 shows that the Type-I error and power of POPPER closely match those of the human participants, with no statistically significant differences given the small sample size. Notably, POPPER completed tasks 9.7 times faster, generated 3.6 times more lines of code, and performed 2.5 times more statistical tests, underscoring its efficiency gains. Qualitative analysis (the right half of Figure 2, where the numbers represent the amount of distinct statistical tests in each category) revealed substantial overlap between human experts and POPPER in both biological falsification experiments (e.g., correlation in gene expression levels, network interactions, eQTL tests) and statistical methods (e.g., permutation, t-test, chi-squared), reinforcing the soundness of POPPER in automating validation tasks.

**Figure 2: Expert human study.** POPPER achieved similar power and Type-I error rates to human experts while significantly reducing task completion time. It also generated more lines of code and conducted more statistical tests. Qualitatively, POPPER and human experts exhibited substantial overlap in both the designed falsification experiments and the statistical methods employed.

**Performance varies across a wide range of LLMs.** Since POPPER must propose meaningful falsification tests and compute valid p-values (per Assumptions 1 and 2), it requires strong reasoning and coding capabilities. We evaluated several LLMs on DiscoveryBench and TargetVal-IL2, including closed-source models (Claude Haiku 3.5, Sonnet 3.5, GPT-4o, o1) and the open-source Llama 3.3 70B. Table 4 shows that higher-capability models are critical: Claude Haiku 3.5 has a high Type-I error, whereas Llama, GPT-4o, Sonnet, and o1 maintained reasonable error control.

**Table 4: Evaluation of various LLM backbones with POPPER.**

| Method | Type I Error ($\alpha = 0.1$) | | Power | |
| :--- | :--- | :--- | :--- | :--- |
| | DiscoveryBench | TargetVal-IL2 | DiscoveryBench | TargetVal-IL2 |
| Claude-Haiku-3.5 | 0.230±0.079 | 0.780±0.120 | 0.844±0.017 | 0.835±0.113 |
| Llama 3.3 70B | 0.147±0.036 | 0.116±0.020 | 0.690±0.027 | 0.515±0.078 |
| GPT-4o | 0.143±0.039 | 0.096±0.043 | 0.730±0.054 | 0.385±0.102 |
| Claude-Sonnet-3.5 | 0.103±0.020 | 0.082±0.046 | 0.638±0.066 | **0.580**\*±0.125 |
| o1 | **0.091**\*±0.015 | **0.031**\*±0.015 | **0.654**\*±0.019 | 0.336±0.121 |

Among them, o1 performed best on DiscoveryBench, and GPT-4o excelled in power for DiscoveryBench, whereas Sonnet led on TargetVal-IL2. These results emphasize the importance of robust reasoning and coding skills for effective hypothesis validation and highlight nuanced performance trade-offs.

### 4.2. Analysis and Discussion

**Qualitative characterization.** We characterize the trajectories of POPPER in Figure 3 (procedure described in Appendix E). In TargetVal, we observe that POPPER designed experiments that span a broad set of biological tests, including protein-protein interaction networks, expression correlation analyses, eQTL regulatory tests, loss-of-function studies, and genetic perturbation tests. During each iteration, the execution agent typically performs up to 14 distinct steps: dataset inspection, preprocessing, model fitting, error handling, statistical testing, visualization, and summarization. Notably, POPPER carefully selects statistical methods based on modeling assumptions (e.g., chi-squared, hypergeometric, Fisher’s, and permutation tests) and often includes well-chosen negative controls. Interestingly, non-parametric tests are most frequent, making them robust to various data distributions. Visualizing the e-value trajectories reveals that evidence against the null accumulates quickly under the alternative while remaining below the nominal threshold under the null, underscoring the rigor and power of sequential testing.

**Figure 3: Characterization of POPPER.** (1) POPPER designs biologically relevant falsification experiments. (2) It performs multiple logical steps to execute the experiment. (3) It employs a wide range of statistical tests. (4) Progression of cumulative e-values across multiple iterations of falsification tests. More details are available in Appendix E.

**Figure 4: Sensitivity analysis.** (1) Empirical Type-I error at various nominal levels $\alpha$. (2) Power and Type-I error at various budgets as a function of the number of maximum tests.

**Sensitivity analysis.** Figure 4 presents the robustness of POPPER under different settings. First, we varied the significance level $\alpha$ and found that POPPER consistently maintained Type-I error control. Second, we examined the effect of increasing the budget (maximum number of tests). While Type-I error remained well-controlled, the power rose with additional tests, indicating that POPPER can accumulate more diverse evidence when given more computational resources. These results demonstrate the scalability of e-values to both small and large numbers of sequential tests, allowing POPPER to achieve higher discovery rates as resources increase.

**Human annotations of falsification test quality.** To assess the implication strength of LLM-generated falsification tests, three authors independently rated 90 randomly selected proposals using the same rubric provided to the ReleCheck agent (Appendix 4). After calibration, the annotators achieved a high inter-rater agreement (Kendall’s $W = 0.91$). The agent’s ratings correlated strongly with human judgments (Spearman’s $\rho = 0.55$, $p = 5 \times 10^{-6}$), though it slightly overestimated the relevance of the implications: it labeled 85% of proposals as “strongly implied,” compared to a 77% pass rate among human evaluators. These findings indicate that while the ReleCheck agent aligns reasonably well with human perspectives, further calibration and domain-specific expertise are needed to enhance the reliability of falsification test selection.

**Error analysis.** We analyzed potential failure modes in POPPER’s hypothesis validation workflow. Using an LLM to categorize errors followed by human inspections, we identified the top reasons for failure: misinterpreted p-values (35.9%), ineffective falsification experiment design (28.1%), falsification test breaks implication (17.2%), and incorrect test implementation (8.6%). Hallucination was minimal (0.8%). More details are provided in Appendix D. Overall, while agentic automation holds promise, our findings highlight areas needing further improvement, guiding future work on more robust hypothesis validation pipelines.

## 5. Related Work

We discuss here related works that are closest to POPPER and provide extended discussion on other related works in Appendix B. LLMs have been widely explored for hypothesis generation, with works focusing on domain-specific ideas (Wang et al., 2024a; Baek et al., 2024; Yang et al., 2024b) and comparisons between AI-generated and expert proposals (Si et al., 2024). Beyond idea generation, some studies refine hypotheses (Honovich et al., 2023; Wang et al., 2024c) or ground them in datasets (Majumder et al., 2024), yet few systematically test free-form hypotheses under rigorous statistical controls. While certain works evaluate LLM-driven experimental protocols (Tian et al., 2024; Gu et al., 2024) or integrate hypothesis and code generation (Li et al., 2024b; Lu et al., 2024; Ifargan et al., 2024; Majumder et al., 2024), they often lack strong error control. Unlike these, POPPER conducts robust statistical validation of both LLM- and human-generated hypotheses through a sequential falsification framework, ensuring reliability. Although Li et al. (2024a) also uses hypothesis testing as a way to challenge language models, POPPER uniquely targets free-form natural language hypotheses and offers rigorous error control.

## 6. Conclusion

We proposed POPPER, an LLM-based framework for validating free-form hypotheses. By integrating a sequential testing paradigm with automated experiment design and execution, POPPER delivers scalable, statistically rigorous hypothesis validation. This work represents an early exploration, and several aspects can be further improved. Refining test relevance and ensuring robust LLM implementations remain challenges. Future work can also extend POPPER to control other error metrics (e.g., false discovery rate), further broadening its utility in scientific discovery and beyond.

**Impact Statement**
This work introduces POPPER, a statistically rigorous agentic framework for hypothesis validation using Large Language Model (LLM) agents. By combining advanced natural language processing capabilities with robust statistical methodologies, POPPER addresses the critical challenge of evaluating and validating hypotheses generated by LLMs, ensuring that only evidence-backed hypotheses guide future research. The broader implications of this work span multiple domains, including biology, economics, and social sciences, where hypothesis generation and validation play a pivotal role in advancing knowledge.

From an ethical perspective, POPPER’s emphasis on rigorous statistical validation and Type-I error control mitigates the risks associated with hallucinated or unsupported hypotheses. This ensures that research resources are directed toward meaningful and plausible hypotheses, reducing the potential for wasted efforts and false conclusions that could mislead scientific progress or policy decisions. Additionally, by automating and accelerating the hypothesis validation process, POPPER democratizes access to high-quality scientific methodologies, enabling smaller research teams and resource-limited institutions to conduct advanced analyses.

**Acknowledgement**
We thank Tatsunori Hashimoto and members of the Jure Leskovec lab for discussions and for providing feedback on our manuscript. We thank the expert user study participants: Michael Bereket, Minta Lu, Peter Pao-Huang, Weixu Wang, Boyang Fu, Hanchen Wang, Hao Xue, Serena Zhang, Yanay Rosen, and Zoe Piran. We also gratefully acknowledge the support of NSF under Nos. OAC-1835598 (CINES), CCF-1918940 (Expeditions), DMS-2327709 (IHBEM), IIS-2403318 (III); Stanford Data Applications Initiative, Wu Tsai Neurosciences Institute, Stanford Institute for Human-Centered AI, Chan Zuckerberg Initiative, Amazon, Genentech, GSK, Hitachi, SAP, and UCB. The content is solely the responsibility of the authors and does not necessarily represent the official views of the funding entities.

## References

Agassi, J. Popper and his popular critics: Thomas kuhn, paul feyerabend and imre lakatos. In *SpringerBriefs in Philosophy*. Springer, 2014. doi: 10.1007/978-3-319-06587-8.

Ajith, A., Xia, M., Chevalier, A., Goyal, T., Chen, D., and Gao, T. Litsearch: A retrieval benchmark for scientific literature search, 2024. URL [https://arxiv.org/abs/2407.18940](https://arxiv.org/abs/2407.18940).

Alet, F., Lopez-Contreras, J., Koppel, J., Nye, M., Solar-Lezama, A., Lozano-Perez, T., Kaelbling, L., and Tenenbaum, J. A large-scale benchmark for few-shot program induction and synthesis. In Meila, M. and Zhang, T. (eds.), *Proceedings of the 38th International Conference on Machine Learning*, volume 139 of *Proceedings of Machine Learning Research*, pp. 175–186. PMLR, 18–24 Jul 2021. URL [https://proceedings.mlr.press/v139/alet21a.html](https://proceedings.mlr.press/v139/alet21a.html).

Baek, J., Jauhar, S. K., Cucerzan, S., and Hwang, S. J. Researchagent: Iterative research idea generation over scientific literature with large language models, 2024. URL [https://arxiv.org/abs/2404.07738](https://arxiv.org/abs/2404.07738).

Benjamini, Y. Selective inference: The silent killer of replicability. 2020.

Benjamini, Y. and Hochberg, Y. Controlling the false discovery rate: a practical and powerful approach to multiple testing. *Journal of the Royal statistical society: series B (Methodological)*, 57(1):289–300, 1995.

Brown, M. B. 400: A method for combining non-independent, one-sided tests of significance. *Biometrics*, pp. 987–992, 1975.

Collaboration, O. S. Estimating the reproducibility of psychological science. *Science*, 349(6251):aac4716, 2015.

Consortium, G. The gtex consortium atlas of genetic regulatory effects across human tissues. *Science*, 369(6509): 1318–1330, 2020.

D’Arcy, M., Hope, T., Birnbaum, L., and Downey, D. Marg: Multi-agent review generation for scientific papers, 2024. URL [https://arxiv.org/abs/2401.04259](https://arxiv.org/abs/2401.04259).

Fisher, R. A. Design of experiments. *British Medical Journal*, 1(3923):554, 1936.

Fisher, R. A. Statistical methods for research workers. In *Breakthroughs in statistics: Methodology and distribution*, pp. 66–70. Springer, 1970.

Gendron, G., Bao, Q., Witbrock, M., and Dobbie, G. Large language models are not strong abstract reasoners, 2024. URL [https://arxiv.org/abs/2305.19555](https://arxiv.org/abs/2305.19555).

Godfrey-Smith, P. *Theory and reality: An introduction to the philosophy of science*. University of Chicago Press, 2009.

Goodman, N. *Fact, Fiction, and Forecast*. Harvard University Press, Cambridge, MA, 1983.

Grünwald, P., de Heide, R., and Koolen, W. M. Safe testing. In *2020 Information Theory and Applications Workshop (ITA)*, pp. 1–54. IEEE, 2020.

Gu, K., Shang, R., Jiang, R., Kuang, K., Lin, R.-J., Lyu, D., Mao, Y., Pan, Y., Wu, T., Yu, J., Zhang, Y., Zhang, T. M., Zhu, L., Merrill, M. A., Heer, J., and Althoff, T. Blade: Benchmarking language model agents for data-driven science, 2024. URL [https://arxiv.org/abs/2408.09667](https://arxiv.org/abs/2408.09667).

Han, S. J., Ransom, K., Perfors, A., and Kemp, C. Inductive reasoning in humans and large language models, 2023. URL [https://arxiv.org/abs/2306.06548](https://arxiv.org/abs/2306.06548).

Honovich, O., Shaham, U., Bowman, S. R., and Levy, O. Instruction induction: From few examples to natural language task descriptions. In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), *Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pp. 1935–1952, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.108. URL [https://aclanthology.org/2023.acl-long.108](https://aclanthology.org/2023.acl-long.108).

Huang, L., Yu, W., Ma, W., Zhong, W., Feng, Z., Wang, H., Chen, Q., Peng, W., Feng, X., Qin, B., et al. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. *ACM Transactions on Information Systems*, 2023.

Ifargan, T., Hafner, L., Kern, M., Alcalay, O., and Kishony, R. Autonomous llm-driven research from data to human-verifiable research papers, 2024. URL [https://arxiv.org/abs/2404.17605](https://arxiv.org/abs/2404.17605).

Ioannidis, J. P. Why most published research findings are false. *PLoS medicine*, 2(8):e124, 2005.

Jun, E., Birchfield, M., De Moura, N., Heer, J., and Just, R. Hypothesis formalization: Empirical findings, software limitations, and design implications. *ACM Transactions on Computer-Human Interaction (TOCHI)*, 29(1):1–28, 2022.

Kuhn, T. S. *The Structure of Scientific Revolutions*. University of Chicago Press, Chicago, 1st edition, 1962.

Lakatos, I. *The Methodology of Scientific Research Programmes*. Cambridge University Press, Cambridge, 1978.

Lehr, S. A., Caliskan, A., Liyanage, S., and Banaji, M. R. Chatgpt as research scientist: Probing gpt’s capabilities as a research librarian, research ethicist, data generator and data predictor, 2024. URL [https://arxiv.org/abs/2406.14765](https://arxiv.org/abs/2406.14765).

Li, M. Y., Vajipey, V., Goodman, N. D., and Fox, E. B. Critical: Critic automation with language models. *arXiv preprint arXiv:2411.06590*, 2024a.

Li, R., Patel, T., Wang, Q., and Du, X. Mlr-copilot: Autonomous machine learning research based on large language models agents, 2024b. URL [https://arxiv.org/abs/2408.14033](https://arxiv.org/abs/2408.14033).

Liang, W., Zhang, Y., Cao, H., Wang, B., Ding, D., Yang, X., Vodrahalli, K., He, S., Smith, D., Yin, Y., McFarland, D., and Zou, J. Can large language models provide useful feedback on research papers? a large-scale empirical analysis, 2023. URL [https://arxiv.org/abs/2310.01783](https://arxiv.org/abs/2310.01783).

Lu, C., Lu, C., Lange, R. T., Foerster, J., Clune, J., and Ha, D. The ai scientist: Towards fully automated open-ended scientific discovery, 2024. URL [https://arxiv.org/abs/2408.06292](https://arxiv.org/abs/2408.06292).

MacArthur, J., Bowler, E., Cerezo, M., Gil, L., Hall, P., Hastings, E., Junkins, H., McMahon, A., Milano, A., Morales, J., et al. The new nhgri-ebi catalog of published genome-wide association studies (gwas catalog). *Nucleic acids research*, 45(D1):D896–D901, 2017.

Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., et al. Self-refine: Iterative refinement with self-feedback. *NeurIPS*, 36, 2024.

Majumder, B. P., Surana, H., Agarwal, D., Mishra, B. D., Meena, A., Prakhar, A., Vora, T., Khot, T., Sabharwal, A., and Clark, P. Discoverybench: Towards data-driven discovery with large language models. *arXiv preprint arXiv:2407.01725*, 2024.

Manning, B. S., Zhu, K., and Horton, J. J. Automated social science: Language models as scientist and subjects, 2024. URL [https://arxiv.org/abs/2404.11794](https://arxiv.org/abs/2404.11794).

Maxwell, N. Popper, kuhn, lakatos and aim-oriented empiricism. *arXiv preprint*, 2012.

Mirchandani, S., Xia, F., Florence, P., Ichter, B., Driess, D., Arenas, M. G., Rao, K., Sadigh, D., and Zeng, A. Large language models as general pattern machines, 2023. URL [https://arxiv.org/abs/2307.04721](https://arxiv.org/abs/2307.04721).

Moskvichev, A., Odouard, V. V., and Mitchell, M. The conceptarc benchmark: Evaluating understanding and generalization in the arc domain, 2023. URL [https://arxiv.org/abs/2305.07141](https://arxiv.org/abs/2305.07141).

Neyman, J. and Pearson, E. S. On the use and interpretation of certain test criteria for purposes of statistical inference part i. *Biometrika*, 20(1-2):175–240, 1928.

Neyman, J. and Pearson, E. S. The testing of statistical hypotheses in relation to probabilities a priori. In *Mathematical proceedings of the Cambridge philosophical society*, volume 29, pp. 492–510. Cambridge University Press, 1933.

Oughtred, R., Stark, C., Breitkreutz, B.-J., Rust, J., Boucher, L., Chang, C., Kolas, N., ODonnell, L., Leung, G., McAdam, R., et al. The biogrid interaction database: 2019 update. *Nucleic acids research*, 47(D1):D529–D541, 2019.

Philosophy Institute. Imre lakatos approach: Bridging popper and kuhn in philosophy of science, 2023. URL [https://philosophy.institute/philosophy-of-science-and-cosmology/imre-lakatos-philosophy-science-bridge/](https://philosophy.institute/philosophy-of-science-and-cosmology/imre-lakatos-philosophy-science-bridge/). Accessed: 2025-01-29.

Popper, K. *The Logic of Scientific Discovery*. Hutchinson, London, 1959.

Popper, K. *The logic of scientific discovery*. Routledge, 2005.

Press, C. U. Normal science and dogmatism, paradigms and progress: Kuhn versus popper and lakatos. 2009.

Press, O., Hochlehnert, A., Prabhu, A., Udandarao, V., Press, O., and Bethge, M. Citeme: Can language models accurately cite scientific claims?, 2024. URL [https://arxiv.org/abs/2407.12861](https://arxiv.org/abs/2407.12861).

Qiu, L., Jiang, L., Lu, X., Sclar, M., Pyatkin, V., Bhagavatula, C., Wang, B., Kim, Y., Choi, Y., Dziri, N., and Ren, X. Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement, 2024. URL [https://arxiv.org/abs/2310.08559](https://arxiv.org/abs/2310.08559).

Ridnik, T., Kredo, D., and Friedman, I. Code generation with alphacodium: From prompt engineering to flow engineering. *arXiv preprint arXiv:2401.08500*, 2024.

Rubin, M. The replication crisis is less of a ”crisis” in lakatos’ philosophy of science. *European Journal for Philosophy of Science*, 15(5), 2025. doi: 10.1007/s13194-024-00629-x.

Schmidt, R., Steinhart, Z., Layeghi, M., Freimer, J. W., Bueno, R., Nguyen, V. Q., Blaeschke, F., Ye, C. J., and Marson, A. Crispr activation and interference screens decode stimulation responses in primary human t cells. *Science*, 375(6580):eabj4008, 2022.

Shafer, G. The language of betting as a strategy for statistical and scientific communication. *arXiv preprint arXiv:1903.06991*, 2019.

Si, C., Yang, D., and Hashimoto, T. Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers, 2024. URL [https://arxiv.org/abs/2409.04109](https://arxiv.org/abs/2409.04109).

Tang, X., Zheng, Z., Li, J., Meng, F., Zhu, S.-C., Liang, Y., and Zhang, M. Large language models are in-context semantic reasoners rather than symbolic reasoners, 2023. URL [https://arxiv.org/abs/2305.14825](https://arxiv.org/abs/2305.14825).

Thompson, W. H. and Skau, S. On the scope of scientific hypotheses. *Royal Society Open Science*, 10(8):230607, 2023.

Tian, M., Gao, L., Zhang, S. D., Chen, X., Fan, C., Guo, X., Haas, R., Ji, P., Krongchon, K., Li, Y., Liu, S., Luo, D., Ma, Y., Tong, H., Trinh, K., Tian, C., Wang, Z., Wu, B., Xiong, Y., Yin, S., Zhu, M., Lieret, K., Lu, Y., Liu, G., Du, Y., Tao, T., Press, O., Callan, J., Huerta, E., and Peng, H. Scicode: A research coding benchmark curated by scientists, 2024. URL [https://arxiv.org/abs/2407.13168](https://arxiv.org/abs/2407.13168).

van Fraassen, B. C. *The Scientific Image*. Clarendon Press, Oxford, 1980.

Vovk, V. and Wang, R. E-values: Calibration, combination and applications. *The Annals of Statistics*, 49(3):1736–1754, 2021.

Wang, Q., Downey, D., Ji, H., and Hope, T. Scimon: Scientific inspiration machines optimized for novelty, 2024a. URL [https://arxiv.org/abs/2305.14259](https://arxiv.org/abs/2305.14259).

Wang, R. and Ramdas, A. False discovery rate control with e-values. *Journal of the Royal Statistical Society Series B: Statistical Methodology*, 84(3):822–852, 2022.

Wang, R., Zelikman, E., Poesia, G., Pu, Y., Haber, N., and Goodman, N. D. Hypothesis search: Inductive reasoning with language models. *ICLR*, 2024b.

Wang, R., Zelikman, E., Poesia, G., Pu, Y., Haber, N., and Goodman, N. D. Hypothesis search: Inductive reasoning with language models, 2024c. URL [https://arxiv.org/abs/2309.05660](https://arxiv.org/abs/2309.05660).

Webb, T., Holyoak, K. J., and Lu, H. Emergent analogical reasoning in large language models, 2023. URL [https://arxiv.org/abs/2212.09196](https://arxiv.org/abs/2212.09196).

Xu, F., Lin, Q., Han, J., Zhao, T., Liu, J., and Cambria, E. Are large language models really good logical reasoners? a comprehensive evaluation and beyond, 2024a. URL [https://arxiv.org/abs/2306.09841](https://arxiv.org/abs/2306.09841).

Xu, Y., Li, W., Vaezipoor, P., Sanner, S., and Khalil, E. B. Llms and the abstraction and reasoning corpus: Successes, failures, and the importance of object-based representations, 2024b. URL [https://arxiv.org/abs/2305.18354](https://arxiv.org/abs/2305.18354).

Yang, Z., Dong, L., Du, X., Cheng, H., Cambria, E., Liu, X., Gao, J., and Wei, F. Language models as inductive reasoners, 2024a. URL [https://arxiv.org/abs/2212.10923](https://arxiv.org/abs/2212.10923).

Yang, Z., Du, X., Li, J., Zheng, J., Poria, S., and Cambria, E. Large language models for automated open-domain scientific hypotheses discovery, 2024b. URL [https://arxiv.org/abs/2309.02726](https://arxiv.org/abs/2309.02726).

Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y. React: Synergizing reasoning and acting in language models. *ICLR*, 2023.

Zhang, X., Xie, Y., Huang, J., Ma, J., Pan, Z., Liu, Q., Xiong, Z., Ergen, T., Shim, D., Lee, H., and Mei, Q. Massw: A new dataset and benchmark tasks for ai-assisted scientific workflows, 2024. URL [https://arxiv.org/abs/2406.06357](https://arxiv.org/abs/2406.06357).

Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging llm-as-a-judge with mt-bench and chatbot arena. *Advances in Neural Information Processing Systems*, 36: 46595–46623, 2023.

Zhou, Y., Liu, H., Srivastava, T., Mei, H., and Tan, C. Hypothesis generation with large language models. *arXiv preprint arXiv:2404.04326*, 2024.