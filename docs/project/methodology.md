## 3 Methodology

OmniRank employs a streamlined multi-agent architecture to perform spectral ranking inference. The system uses a fixed pipeline triggered by user data upload, with two specialized agents handling data processing and analysis, respectively.

### 3.1 Overview

OmniRank is structured around three core components: the **Data Agent**, **Engine Orchestrator**, and **Analyst Agent**. The Data Agent handles data preprocessing and parameter inference. The Engine Orchestrator invokes the spectral ranking computation. The Analyst Agent generates reports, visualizations, and handles user Q&A by combining ranking results with spectral ranking knowledge. Error diagnosis is also performed by the Analyst Agent.

When users upload comparison data, the workflow proceeds as follows: The Data Agent preprocesses the data and infers the semantic schema (including `bigbetter`, items, and indicators). **Users then verify and refine these settings via the system's interactive configuration panel.** Upon confirmation, the Engine Orchestrator invokes `spectral_ranking_step1.R` to compute rankings. If errors occur, the Analyst Agent diagnoses the issue and requests corrections. Upon successful computation, the Analyst Agent generates reports, visualizations, and supports ongoing Q&A with users. The workflow is formalized in Algorithm 1.

**Algorithm 1** OmniRank Workflow.

**Require:** $Da$: Data Agent, $Eo$: Engine Orchestrator, $An$: Analyst Agent, $M$: Session Memory
**Require:** $d$: Dataset provided by user, $T$: Maximum number of attempts

// Phase 1: Data Processing & Configuration
1: $schema \leftarrow Da$.infer\_schema($d$)  $\quad \triangleright$ Infer BigBetter, Items, Indicator
2: $params \leftarrow Eo$.configure($schema$, $User\_Input$)  $\quad \triangleright$ Interactive user verification (Select indicator values)
3: $M$.update_data_state($schema$, $params$)

// Phase 2: Computation (Dynamic Workflow)
4: $n \leftarrow 0$
5: $step1\_result \leftarrow Eo$.execute\_step1($params$)  $\quad \triangleright$ Execute spectral_ranking_step1.R
6: **if** $Eo$.should\_refine($step1\_result$.metadata) **then**  $\quad \triangleright$ Check heterogeneity index
7:     $result \leftarrow Eo$.execute\_step2($params$, $step1\_result$)  $\quad \triangleright$ Execute spectral_ranking_step2.R
8: **else**
9:     $result \leftarrow step1\_result$
10: **end if**

// Phase 3: Error Handling (Analyst diagnoses)
11: **while** $result = ERROR$ **and** $n < T$ **do**
12:    $n \leftarrow n + 1$
13:    $error\_type, suggestions \leftarrow An$.diagnose($result$.error, $M$.trace)
14:    // ... [Error handling logic same as before] ...
15:    $result \leftarrow Eo$.re\_execute($params$)
16: **end while**

// Phase 4: Output Generation
// ... [Same as before] ...

### 3.2 Data Agent
    
The Data Agent acts as the intelligent interface between user data and the spectral engine, performing two critical functions to ensure data readiness and semantic understanding.

**Function 1: Format Recognition & Standardization & Validation.** The agent automatically identifies the structure of uploaded data (e.g., Pointwise, Pairwise, Multiway) and validates its suitability for spectral ranking. The recognition component adapts to diverse input structures, preserving original data fidelity while ensuring compatibility with the spectral engine (`spectral_ranking_step1.R`). The validation component performs targeted sanity checks:
- **Sparsity Warnings**: Issued when comparison counts fall below the theoretical threshold $M < n \log n$, where $M$ denotes total pairwise comparisons and $n$ the number of items. This threshold, established by Fan et al. (2023), represents the minimum sample complexity required for consistent spectral estimation, analogous to the coupon collector bound.
- **Connectivity Warnings**: Issued when the comparison graph is disjoint (verified using `networkx`), indicating that global rankings cannot be computed and results will only be meaningful within connected components.
- **Critical Errors**: Issued when required ranking columns are missing or fewer than two items are present, blocking execution entirely.
Data classified as `invalid` (e.g., insufficient numeric columns, all-text data) is rejected with explanatory feedback generated by the LLM in plain language. This tiered approach ensures users understand data limitations without blocking valid exploratory analysis.

**Function 2: Semantic Schema Inference.** Beyond format recognition, the agent infers the semantic role of data components to facilitate flexible downstream analysis. This includes:
- **Preference Direction (`bigbetter`)**: Inferring whether higher values indicate better performance (e.g., accuracy) or worse performance (e.g., latency) using both macro-level column naming patterns and micro-level value distributions.
- **Ranking Items Identification**: Identifying the entities to be ranked (e.g., "ChatGPT", "Claude").
- **Ranking Indicators Identification**: Identifying categorical dimensions (e.g., "Task"). CRITICAL: The agent extracts at most ONE indicator column to maintain analysis focus.
- **Indicator Values Extraction**: Extracting unique semantic groups (e.g., "code", "math") within the selected indicator.
This metadata enables the Engine Orchestrator to expose precise control parameters to the user, allowing for customized rankings based on specific items or indicator segments.

### 3.3 Engine Orchestrator

The Engine Orchestrator is a **deterministic system component** that manages the transition from data schema to statistical computation. It ensures execution reliability through interactive configuration and robust resource management.

**Function 1: Interactive Configuration Management.** The orchestrator empowers users to fine-tune the analysis by exposing the metadata inferred by the Data Agent. Through an interactive control panel, users can:
- **Parameter Adjustment**: Verify and modify the **Preference Direction** (`bigbetter`), select specific **Ranking Items** subsets, or choose distinct **Ranking Indicators** for analysis.
- **Advanced Options**: Configure statistical parameters such as **Bootstrap Iterations** (default to 2000 for robust CIs) and **Random Seed** (default to 42 for reproducibility).
This ensures that the final execution aligns precisely with user intent, even if the Data Agent's initial inferences require adjustment.

**Function 2: Robust Engine Execution with Dynamic Orchestration.** It encapsulates the spectral ranking logic, executing it within isolated processes. Crucially, the orchestrator implements the **Two-Step Spectral Method** logic from *Fan et al. (2023)*:
1. **Initial Estimation**: It first invokes the vanilla spectral engine (`spectral_ranking_step1.R`) using simple weighting ($f(A_l)=|A_l|$) to obtain consistent initial estimates.
2. **Diagnostic Check & Adaptive Refinement**: It evaluates three statistical criteria autonomously:
   - **Sparsity Gatekeeper**: Checks if data sufficiency meets the theoretical threshold ($M \geq n \log n$, i.e., sparsity\_ratio $\geq 1.0$). This is the optimal sample complexity requirement (coupon collector bound). If data is too sparse, refinement is skipped to maintain stability.
   - **Heterogeneity Trigger**: Checks if comparison counts are highly uneven (CV > 0.5). When heterogeneity is high, the vanilla spectral method with uniform weights is suboptimal; optimal weighting provides efficiency gains.
   - **Uncertainty Trigger**: Checks if the top-5 items have wide confidence intervals relative to the number of items (CI\_width / n > 20%). Wide CIs indicate high variance in Step 1 estimates. Since CI width $\propto \sqrt{\text{Var}}$, Step 2's optimal weighting can reduce variance and narrow confidence intervals (Theorem 2, Remark 6 in Fan et al., 2023).
   If data is sufficient and either trigger is activated, the orchestrator automatically executes the second estimation step (`spectral_ranking_step2.R`) using optimal weights ($f(A_l) \propto \sum e^{\hat{\theta}_u}$), which achieves the same asymptotic efficiency as MLE (Cramér-Rao lower bound).
This dynamic workflow ensures that users receive the most statistically efficient estimates without needing to understand the underlying complexity of weighting schemes.

### 3.4 Analyst Agent

The Analyst Agent is responsible for all post-computation tasks: report generation, visualization, user Q&A, and error diagnosis. Upon receiving ranking results from the Engine Orchestrator, the Analyst Agent performs two critical functions.

**Function 1: Report & Visualization Generation.** The agent transforms raw ranking results into comprehensive, publication-ready outputs through two complementary processes:
- **Report Synthesis**: Generates structured reports containing: executive summary highlighting key findings and top-ranked items, detailed rankings with confidence intervals and statistical significance indicators, methodology notes explaining the spectral approach and any two-step refinement applied, and actionable insights tailored to the data domain. Reports are rendered in both markdown (for quick review) and PDF formats (for formal documentation).
- **Visualization Production**: Creates a suite of interactive and static visualizations including: (1) rank plots with confidence interval error bars showing uncertainty in rankings, (2) pairwise comparison heatmaps revealing win/loss patterns between items, and (3) preference score distributions displaying the estimated $\theta$ values.

**Function 2: Interactive User Q&A.** The agent handles follow-up questions from users by combining session memory with external spectral ranking knowledge. The session memory architecture maintains three components within each analysis session:
- **Data State**: Current data schema, validation results, and inferred parameters
- **Execution Trace**: Log of all computation invocations for error diagnosis
- **Conversation Context**: User intent history enabling follow-up queries

This architecture enables natural conversational workflows—for example, after computing initial rankings, a user can simply ask "Is model A significantly better than model B?" without re-uploading data or restating the analysis context. The agent interprets such queries by retrieving relevant confidence intervals from the results and applying spectral ranking theory to provide statistically grounded answers.

### 3.5 Agent System Prompts

We present the system prompts design for the two reasoning agents: the **Data Agent** and the **Analyst Agent**. The Engine Orchestrator, being a deterministic component, does not utilize LLM prompts.

Each agent incorporates a **Knowledge Layer** that embeds domain expertise directly into its system prompt, following OpenAI's recommended Structured System Instructions pattern. This enables expert-level theoretical grounding without requiring users to provide specialized knowledge. For example, the Analyst Agent's knowledge layer includes spectral ranking theory concepts such as confidence interval interpretation, the two-step estimation method, and heterogeneity thresholds.

**Figure 2: Data Agent Prompt Strategy.**
![Data Agent Prompt](https://placehold.co/600x400?text=Data+Agent+Prompt+Placeholder)

**Figure 3: Engine Orchestrator Prompt Strategy.**
![Engine Orchestrator Prompt](https://placehold.co/600x400?text=Engine+Orchestrator+Prompt+Placeholder)

**Figure 4: Analyst Agent Prompt Strategy.**
![Analyst Agent Prompt](https://placehold.co/600x400?text=Analyst+Prompt+Placeholder)

### 3.6 User interface

OmniRank provides an accessible, chat-based interface that guides users through a three-stage analysis workflow:

1.  **Data Analysis**: Users upload raw datasets, and the Data Agent automatically infers the structure and semantic schema.
2.  **Interactive Configuration**: The interface presents the inferred settings (e.g., preference direction, ranking items) in a visual control panel. Users confirm or adjust these settings, which are then validated and passed to the Engine Orchestrator for deterministic execution.
3.  **Results & Exploration**: The Analyst Agent presents the final rankings, visualizations, and a natural language summary. Users can then ask follow-up questions (e.g., "Is the top model significantly better than the second?") to explore the results deeply without restarting the session.

This design enables experts and non-experts alike to leverage spectral ranking methods with confidence and precision.

To summarize, the Data Agent, Engine Orchestrator, and Analyst Agent collectively ensure the reliability and accessibility of OmniRank through a streamlined fixed pipeline. The modular architecture makes OmniRank flexible for diverse ranking applications across social and natural sciences.

### 3.7 Spectral ranking inference engine

In general, the spectral ranking approach transforms comparison data into a Markov chain over the $n$ items and leverages its stationary distribution to infer item scores. We assume there are $n$ items to be ranked, and the preference scores of a given group of $n$ items can be parameterized as $\boldsymbol{\theta}^* = (\theta_1^*, \ldots, \theta_n^*)^T$ such that for any choice set $A$ and item $i \in A$ we have:

$$P(i \text{ wins among } A) = \frac{e^{\theta_i^*}}{\sum_{k \in A} e^{\theta_k^*}}$$

For a general comparison model of the $n$ items, we are given a collection of comparisons and outcomes $\{(c_l, A_l)\}_{l \in D}$ where $c_l$ denotes the selected item over the choice set $A_l$. We construct a directed comparison graph where each item corresponds to a state, and define a transition matrix $P$ with entries:

$$P_{ij} = \frac{1}{d} \sum_{l \in W_j \cap L_i} \frac{1}{f(A_l)}$$

where $W_j = \{l \in D | j \in A_l, c_l = j\}$ and $L_i = \{l \in D | i \in A_l, c_l \neq i\}$ are index sets for comparisons where $j$ wins and $i$ loses, respectively. This matrix characterizes a Markov chain whose long-term visiting frequency reflects the underlying preference structure.

The stationary distribution $\hat{\pi}$ of this chain—obtained as the leading eigenvector of $P^T$ associated with eigenvalue 1—serves as the spectral score for each item. Compared to likelihood-based models such as Bradley-Terry-Luce (BTL) or Plackett-Luce (PL), which require iterative optimization, spectral methods are computationally simpler: only a single eigen-decomposition is needed, with $O(n^3)$ complexity.

The spectral scores are transformed into estimated preference parameters via:

$$\tilde{\theta}_i = \log \hat{\pi}_i - \frac{1}{n} \sum_{k=1}^{n} \log \hat{\pi}_k$$

Finally, the inferred ranking is produced by sorting $\tilde{\theta}_i$ in descending order. For uncertainty quantification, we employ the Gaussian multiplier bootstrap method to construct confidence intervals for ranks, as detailed in Fan et al. (2023).
